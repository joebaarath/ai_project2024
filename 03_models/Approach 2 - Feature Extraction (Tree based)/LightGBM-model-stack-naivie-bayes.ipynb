{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8092275,"sourceType":"datasetVersion","datasetId":4777705}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom gensim.models import KeyedVectors\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nimport lightgbm as lgb\nfrom sklearn.naive_bayes import GaussianNB  # Import Naive Bayes\nfrom sklearn.ensemble import StackingClassifier\n\n# Load the dataset\ntrain_data = pd.read_csv('/kaggle/input/lemma-balanced/lemmatized_dataset_final_balanced_train.csv')\nvalid_data = pd.read_csv('/kaggle/input/lemma-balanced/lemmatized_dataset_final_balanced_validation.csv')\ntest_data = pd.read_csv('/kaggle/input/lemma-balanced/lemmatized_dataset_final_balanced_test.csv')\n\n# Display the first few rows of the dataset\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T05:01:54.846742Z","iopub.execute_input":"2024-04-15T05:01:54.847124Z","iopub.status.idle":"2024-04-15T05:01:55.380972Z","shell.execute_reply.started":"2024-04-15T05:01:54.847091Z","shell.execute_reply":"2024-04-15T05:01:55.380078Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                                            Headline  Body ID     Stance  \\\n0  dna test confirm lebanon is holding isi leader...     2042  unrelated   \n1  somalia shebab chief ahmed abdi godane likely ...     1610    discuss   \n2  dna test prove lebanon is holding isi chief al...     1468   disagree   \n3  the pumpkinspice condom is just a figment of y...     1253  unrelated   \n4  u probing claim isi fighter seized airdropped ...      465    discuss   \n\n                                         articleBody  stance_cat  \n0  there is a story currently making the round ab...           3  \n1  ahmed abdi godane the leader of al shabab the ...           2  \n2  an iraqi official denied that a woman detained...           1  \n3  the united state department of defense said on...           3  \n4  the pentagon admitted on wednesday that isi di...           2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Headline</th>\n      <th>Body ID</th>\n      <th>Stance</th>\n      <th>articleBody</th>\n      <th>stance_cat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>dna test confirm lebanon is holding isi leader...</td>\n      <td>2042</td>\n      <td>unrelated</td>\n      <td>there is a story currently making the round ab...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>somalia shebab chief ahmed abdi godane likely ...</td>\n      <td>1610</td>\n      <td>discuss</td>\n      <td>ahmed abdi godane the leader of al shabab the ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>dna test prove lebanon is holding isi chief al...</td>\n      <td>1468</td>\n      <td>disagree</td>\n      <td>an iraqi official denied that a woman detained...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>the pumpkinspice condom is just a figment of y...</td>\n      <td>1253</td>\n      <td>unrelated</td>\n      <td>the united state department of defense said on...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>u probing claim isi fighter seized airdropped ...</td>\n      <td>465</td>\n      <td>discuss</td>\n      <td>the pentagon admitted on wednesday that isi di...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"valid_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T05:01:55.382867Z","iopub.execute_input":"2024-04-15T05:01:55.383261Z","iopub.status.idle":"2024-04-15T05:01:55.393968Z","shell.execute_reply.started":"2024-04-15T05:01:55.383231Z","shell.execute_reply":"2024-04-15T05:01:55.393170Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                                            Headline  Body ID     Stance  \\\n0  james foley american journalist james wright f...     1336      agree   \n1  yous hostage luke somers killed during yemen r...     1119    discuss   \n2  the internet tried to make axl rose it latest ...     1857  unrelated   \n3  conde nast rat problem at one world trade cent...      821      agree   \n4  video marine survives taliban sniper headshot ...     1256  unrelated   \n\n                                         articleBody  stance_cat  \n0  two yous official say they believe american jo...           0  \n1  a britishborn photographer held hostage by al ...           2  \n2  those caught with the same moniker a the dicta...           3  \n3  the rat infestation in vogue new 1 world trade...           0  \n4  the pentagon ha confirmed that ahmed abdi goda...           3  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Headline</th>\n      <th>Body ID</th>\n      <th>Stance</th>\n      <th>articleBody</th>\n      <th>stance_cat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>james foley american journalist james wright f...</td>\n      <td>1336</td>\n      <td>agree</td>\n      <td>two yous official say they believe american jo...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>yous hostage luke somers killed during yemen r...</td>\n      <td>1119</td>\n      <td>discuss</td>\n      <td>a britishborn photographer held hostage by al ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>the internet tried to make axl rose it latest ...</td>\n      <td>1857</td>\n      <td>unrelated</td>\n      <td>those caught with the same moniker a the dicta...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>conde nast rat problem at one world trade cent...</td>\n      <td>821</td>\n      <td>agree</td>\n      <td>the rat infestation in vogue new 1 world trade...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>video marine survives taliban sniper headshot ...</td>\n      <td>1256</td>\n      <td>unrelated</td>\n      <td>the pentagon ha confirmed that ahmed abdi goda...</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T05:01:55.394921Z","iopub.execute_input":"2024-04-15T05:01:55.395157Z","iopub.status.idle":"2024-04-15T05:01:55.411564Z","shell.execute_reply.started":"2024-04-15T05:01:55.395137Z","shell.execute_reply":"2024-04-15T05:01:55.410688Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"                                            Headline  Body ID     Stance  \\\n0  god is a woman priest who died for 48 minute c...     1227   disagree   \n1  update internet report of ebola outbreak in pu...      958  unrelated   \n2  british hostage david haines beheaded by islam...     2034  unrelated   \n3  video messaging service verifies timing of cnn...     1435  unrelated   \n4  omar gonzalez white house fencejumper made it ...      917  unrelated   \n\n                                         articleBody  stance_cat  \n0  a supposed catholic priest claim of seeing god...           1  \n1  an animal lover from norfolk splashed hundred ...           3  \n2  when a report went viral that nbc meteorologis...           3  \n3  apple may be planning to hold a special event ...           3  \n4  source detroit free press yesterday we reporte...           3  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Headline</th>\n      <th>Body ID</th>\n      <th>Stance</th>\n      <th>articleBody</th>\n      <th>stance_cat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>god is a woman priest who died for 48 minute c...</td>\n      <td>1227</td>\n      <td>disagree</td>\n      <td>a supposed catholic priest claim of seeing god...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>update internet report of ebola outbreak in pu...</td>\n      <td>958</td>\n      <td>unrelated</td>\n      <td>an animal lover from norfolk splashed hundred ...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>british hostage david haines beheaded by islam...</td>\n      <td>2034</td>\n      <td>unrelated</td>\n      <td>when a report went viral that nbc meteorologis...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>video messaging service verifies timing of cnn...</td>\n      <td>1435</td>\n      <td>unrelated</td>\n      <td>apple may be planning to hold a special event ...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>omar gonzalez white house fencejumper made it ...</td>\n      <td>917</td>\n      <td>unrelated</td>\n      <td>source detroit free press yesterday we reporte...</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_data['stance_cat'] = train_data['Stance'].map({'agree':0,'disagree':1,'discuss':2,'unrelated':3}).astype(int)\ntrain_data['Stance'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T05:01:55.413849Z","iopub.execute_input":"2024-04-15T05:01:55.414456Z","iopub.status.idle":"2024-04-15T05:01:55.431722Z","shell.execute_reply.started":"2024-04-15T05:01:55.414412Z","shell.execute_reply":"2024-04-15T05:01:55.430874Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"Stance\nunrelated    10742\ndiscuss       7127\nagree         2942\ndisagree       672\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"valid_data['stance_cat'] = valid_data['Stance'].map({'agree':0,'disagree':1,'discuss':2,'unrelated':3}).astype(int)\nvalid_data['Stance'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T05:01:55.432853Z","iopub.execute_input":"2024-04-15T05:01:55.433216Z","iopub.status.idle":"2024-04-15T05:01:55.443926Z","shell.execute_reply.started":"2024-04-15T05:01:55.433182Z","shell.execute_reply":"2024-04-15T05:01:55.443061Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Stance\nunrelated    1343\ndiscuss       891\nagree         368\ndisagree       84\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"test_data['stance_cat'] = test_data['Stance'].map({'agree':0,'disagree':1,'discuss':2,'unrelated':3}).astype(int)\ntest_data['Stance'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T05:01:55.445160Z","iopub.execute_input":"2024-04-15T05:01:55.446054Z","iopub.status.idle":"2024-04-15T05:01:55.458261Z","shell.execute_reply.started":"2024-04-15T05:01:55.446031Z","shell.execute_reply":"2024-04-15T05:01:55.457432Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"Stance\nunrelated    1342\ndiscuss       891\nagree         368\ndisagree       84\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"import torch\n\n# Check if CUDA (GPU support) is available\nif torch.cuda.is_available():\n    print('CUDA is available. GPU will be used.')\nelse:\n    print('CUDA is not available. CPU will be used.')","metadata":{"execution":{"iopub.status.busy":"2024-04-15T05:01:55.459361Z","iopub.execute_input":"2024-04-15T05:01:55.459883Z","iopub.status.idle":"2024-04-15T05:01:55.468030Z","shell.execute_reply.started":"2024-04-15T05:01:55.459853Z","shell.execute_reply":"2024-04-15T05:01:55.467301Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"CUDA is available. GPU will be used.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Updated preprocess_text function\ndef preprocess_text(text):\n    # Tokenize text\n    tokens = word_tokenize(text)\n    return \" \".join(tokens)\n\n# Applying the updated preprocess_text function\ntrain_data['headline_processed'] = train_data['Headline'].apply(preprocess_text)\ntrain_data['body_processed'] = train_data['articleBody'].apply(preprocess_text)\n\nvalid_data['headline_processed'] = valid_data['Headline'].apply(preprocess_text)\nvalid_data['body_processed'] = valid_data['articleBody'].apply(preprocess_text)\n\ntest_data['headline_processed'] = test_data['Headline'].apply(preprocess_text)\ntest_data['body_processed'] = test_data['articleBody'].apply(preprocess_text)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T05:01:55.468993Z","iopub.execute_input":"2024-04-15T05:01:55.469241Z","iopub.status.idle":"2024-04-15T05:03:16.056307Z","shell.execute_reply.started":"2024-04-15T05:01:55.469220Z","shell.execute_reply":"2024-04-15T05:03:16.055479Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Count how many times each gram appears in the headline and body\nheadline_unigrams = train_data['headline_processed'].apply(lambda x: x.split())\nbody_unigrams = train_data['body_processed'].apply(lambda x: x.split())\n\n# Count the number of unique grams in the headline and body\nheadline_unique_counts = headline_unigrams.apply(lambda x: len(set(x)))\nbody_unique_counts = body_unigrams.apply(lambda x: len(set(x)))\n\n# Compute the ratio between the count of grams in the headline and body\nratio_counts = headline_unique_counts / body_unique_counts\n\n# Compute overlapping count of grams between headline and body\noverlapping_counts = []\nfor headline, body in zip(headline_unigrams, body_unigrams):\n    overlapping_counts.append(len(set(headline) & set(body)))\n\n# Normalize overlapping count by the number of grams in the headline\nnormalized_overlapping_counts = [oc / len(h) if len(h) > 0 else 0 for oc, h in zip(overlapping_counts, headline_unigrams)]\n\n# Create DataFrame for count features\ncount_features_train = pd.DataFrame({\n    'headline_unique_counts': headline_unique_counts,\n    'body_unique_counts': body_unique_counts,\n    'ratio_counts': ratio_counts,\n    'overlapping_counts': overlapping_counts,\n    'normalized_overlapping_counts': normalized_overlapping_counts\n})\n\n#Validation set \n# Count how many times each gram appears in the headline and body\nheadline_unigrams = valid_data['headline_processed'].apply(lambda x: x.split())\nbody_unigrams = valid_data['body_processed'].apply(lambda x: x.split())\n\n# Count the number of unique grams in the headline and body\nheadline_unique_counts = headline_unigrams.apply(lambda x: len(set(x)))\nbody_unique_counts = body_unigrams.apply(lambda x: len(set(x)))\n\n# Compute the ratio between the count of grams in the headline and body\nratio_counts = headline_unique_counts / body_unique_counts\n\n# Compute overlapping count of grams between headline and body\noverlapping_counts = []\nfor headline, body in zip(headline_unigrams, body_unigrams):\n    overlapping_counts.append(len(set(headline) & set(body)))\n\n# Normalize overlapping count by the number of grams in the headline\nnormalized_overlapping_counts = [oc / len(h) if len(h) > 0 else 0 for oc, h in zip(overlapping_counts, headline_unigrams)]\n\n# Create DataFrame for count features\ncount_features_valid = pd.DataFrame({\n    'headline_unique_counts': headline_unique_counts,\n    'body_unique_counts': body_unique_counts,\n    'ratio_counts': ratio_counts,\n    'overlapping_counts': overlapping_counts,\n    'normalized_overlapping_counts': normalized_overlapping_counts\n})\n\n# Applying the same process for test data\n# Count how many times each gram appears in the headline and body\ntest_headline_unigrams = test_data['headline_processed'].apply(lambda x: x.split())\ntest_body_unigrams = test_data['body_processed'].apply(lambda x: x.split())\n\n# Count the number of unique grams in the headline and body\ntest_headline_unique_counts = test_headline_unigrams.apply(lambda x: len(set(x)))\ntest_body_unique_counts = test_body_unigrams.apply(lambda x: len(set(x)))\n\n# Compute the ratio between the count of grams in the headline and body\ntest_ratio_counts = test_headline_unique_counts / test_body_unique_counts\n\n# Compute overlapping count of grams between headline and body\ntest_overlapping_counts = []\nfor headline, body in zip(test_headline_unigrams, test_body_unigrams):\n    test_overlapping_counts.append(len(set(headline) & set(body)))\n\n# Normalize overlapping count by the number of grams in the headline\ntest_normalized_overlapping_counts = [oc / len(h) if len(h) > 0 else 0 for oc, h in zip(test_overlapping_counts, test_headline_unigrams)]\n\n# Create DataFrame for count features for test data\ncount_features_test = pd.DataFrame({\n    'headline_unique_counts': test_headline_unique_counts,\n    'body_unique_counts': test_body_unique_counts,\n    'ratio_counts': test_ratio_counts,\n    'overlapping_counts': test_overlapping_counts,\n    'normalized_overlapping_counts': test_normalized_overlapping_counts\n})","metadata":{"execution":{"iopub.status.busy":"2024-04-15T05:03:16.057567Z","iopub.execute_input":"2024-04-15T05:03:16.057891Z","iopub.status.idle":"2024-04-15T05:03:19.186773Z","shell.execute_reply.started":"2024-04-15T05:03:16.057866Z","shell.execute_reply":"2024-04-15T05:03:19.185904Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)\n\n# TF-IDF Vectorization for training data\nheadline_tfidf_train = tfidf_vectorizer.fit_transform(train_data['headline_processed'])\nbody_tfidf_train = tfidf_vectorizer.transform(train_data['body_processed'])\n\n# Compute cosine similarity between headline and body vectors for training data\ncosine_similarities_train = []\nfor headline_vec, body_vec in zip(headline_tfidf_train, body_tfidf_train):\n    cosine_similarities_train.append(cosine_similarity(headline_vec.reshape(1, -1), body_vec.reshape(1, -1))[0][0])\n\n# Create DataFrame for TF-IDF features for training data\ntfidf_features_train = pd.DataFrame({\n    'cosine_similarity': cosine_similarities_train\n})\n\nheadline_tfidf_valid = tfidf_vectorizer.fit_transform(valid_data['headline_processed'])\nbody_tfidf_valid = tfidf_vectorizer.transform(valid_data['body_processed'])\n\n# Compute cosine similarity between headline and body vectors for training data\ncosine_similarities_valid = []\nfor headline_vec, body_vec in zip(headline_tfidf_valid, body_tfidf_valid):\n    cosine_similarities_valid.append(cosine_similarity(headline_vec.reshape(1, -1), body_vec.reshape(1, -1))[0][0])\n\n# Create DataFrame for TF-IDF features for training data\ntfidf_features_valid = pd.DataFrame({\n    'cosine_similarity': cosine_similarities_valid\n})\n\n\n# TF-IDF Vectorization for testing data\nheadline_tfidf_test = tfidf_vectorizer.transform(test_data['headline_processed'])\nbody_tfidf_test = tfidf_vectorizer.transform(test_data['body_processed'])\n\n# Compute cosine similarity between headline and body vectors for testing data\ncosine_similarities_test = []\nfor headline_vec, body_vec in zip(headline_tfidf_test, body_tfidf_test):\n    cosine_similarities_test.append(cosine_similarity(headline_vec.reshape(1, -1), body_vec.reshape(1, -1))[0][0])\n\n# Create DataFrame for TF-IDF features for testing data\ntfidf_features_test = pd.DataFrame({\n    'cosine_similarity': cosine_similarities_test\n})","metadata":{"execution":{"iopub.status.busy":"2024-04-15T05:03:19.189861Z","iopub.execute_input":"2024-04-15T05:03:19.190180Z","iopub.status.idle":"2024-04-15T05:03:47.672703Z","shell.execute_reply.started":"2024-04-15T05:03:19.190139Z","shell.execute_reply":"2024-04-15T05:03:47.671839Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\n# Perform Singular Value Decomposition (SVD) on TF-IDF features for train dataset\nsvd_train = TruncatedSVD(n_components=100)  # Adjust the number of components as needed\nheadline_svd_train = svd_train.fit_transform(headline_tfidf_train)\nbody_svd_train = svd_train.transform(body_tfidf_train)\n\n# Compute cosine similarity between SVD features of headline and body for train dataset\ncosine_sim_svd_train = []\nfor h_svd, b_svd in zip(headline_svd_train, body_svd_train):\n    cosine_sim_svd_train.append(cosine_similarity(h_svd.reshape(1, -1), b_svd.reshape(1, -1))[0][0])\n\n# Create DataFrame for SVD features for train dataset\nsvd_features_train = pd.DataFrame({\n    'cosine_similarity_svd': cosine_sim_svd_train\n})\n\n# Perform Singular Value Decomposition (SVD) on TF-IDF features for train dataset\nsvd_valid = TruncatedSVD(n_components=100)  # Adjust the number of components as needed\nheadline_svd_valid = svd_valid.fit_transform(headline_tfidf_valid)\nbody_svd_valid = svd_valid.transform(body_tfidf_valid)\n\n# Compute cosine similarity between SVD features of headline and body for train dataset\ncosine_sim_svd_valid = []\nfor h_svd, b_svd in zip(headline_svd_valid, body_svd_valid):\n    cosine_sim_svd_valid.append(cosine_similarity(h_svd.reshape(1, -1), b_svd.reshape(1, -1))[0][0])\n\n# Create DataFrame for SVD features for train dataset\nsvd_features_valid = pd.DataFrame({\n    'cosine_similarity_svd': cosine_sim_svd_valid\n})\n\n# Perform Singular Value Decomposition (SVD) on TF-IDF features for test dataset\nsvd_test = TruncatedSVD(n_components=100)  # Adjust the number of components as needed\nheadline_svd_test = svd_test.fit_transform(headline_tfidf_test)\nbody_svd_test = svd_test.transform(body_tfidf_test)\n\n# Compute cosine similarity between SVD features of headline and body for test dataset\ncosine_sim_svd_test = []\nfor h_svd, b_svd in zip(headline_svd_test, body_svd_test):\n    cosine_sim_svd_test.append(cosine_similarity(h_svd.reshape(1, -1), b_svd.reshape(1, -1))[0][0])\n\n# Create DataFrame for SVD features for test dataset\nsvd_features_test = pd.DataFrame({\n    'cosine_similarity_svd': cosine_sim_svd_test\n})","metadata":{"execution":{"iopub.status.busy":"2024-04-15T05:03:47.673950Z","iopub.execute_input":"2024-04-15T05:03:47.674265Z","iopub.status.idle":"2024-04-15T05:03:56.230403Z","shell.execute_reply.started":"2024-04-15T05:03:47.674239Z","shell.execute_reply":"2024-04-15T05:03:56.229564Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Install Kaggle API\n!pip install kaggle\n\n# Set up Kaggle API credentials (replace 'username' and 'key' with your Kaggle username and API key)\n!mkdir ~/.kaggle\n!echo '{\"username\":\"joshidevanshi\",\"key\":\"4e038483c682fdbfb691f4fb95f5a416\"}' > ~/.kaggle/kaggle.json\n\n# Download the Google News Word2Vec embeddings dataset\n!kaggle datasets download -d leadbest/googlenewsvectorsnegative300\n\n# Unzip the downloaded file\n!unzip googlenewsvectorsnegative300.zip","metadata":{"execution":{"iopub.status.busy":"2024-04-15T05:03:56.231545Z","iopub.execute_input":"2024-04-15T05:03:56.232179Z","iopub.status.idle":"2024-04-15T05:15:40.149310Z","shell.execute_reply.started":"2024-04-15T05:03:56.232152Z","shell.execute_reply":"2024-04-15T05:15:40.148109Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Requirement already satisfied: kaggle in /opt/conda/lib/python3.10/site-packages (1.6.8)\nRequirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kaggle) (1.16.0)\nRequirement already satisfied: certifi>=2023.7.22 in /opt/conda/lib/python3.10/site-packages (from kaggle) (2024.2.2)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kaggle) (2.9.0.post0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kaggle) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kaggle) (4.66.1)\nRequirement already satisfied: python-slugify in /opt/conda/lib/python3.10/site-packages (from kaggle) (8.0.4)\nRequirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from kaggle) (1.26.18)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from kaggle) (6.1.0)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->kaggle) (0.5.1)\nRequirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.10/site-packages (from python-slugify->kaggle) (1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle) (3.6)\nmkdir: cannot create directory '/root/.kaggle': File exists\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\ngooglenewsvectorsnegative300.zip: Skipping, found more recently modified local copy (use --force to force download)\nArchive:  googlenewsvectorsnegative300.zip\nreplace GoogleNews-vectors-negative300.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n","output_type":"stream"}]},{"cell_type":"code","source":"from gensim.models import KeyedVectors\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load pre-trained word vectors\nword_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n\n# Function to generate word vectors for a given text\ndef generate_word_vectors(text):\n    vectors = []\n    tokens = text.split()\n    for token in tokens:\n        if token in word_vectors:\n            vectors.append(word_vectors[token])\n    if vectors:\n        return np.mean(vectors, axis=0)\n    else:\n        return np.zeros(word_vectors.vector_size) \n\n# Generate word vectors for headline and body for train dataset\nheadline_word2vec_train = train_data['headline_processed'].apply(generate_word_vectors)\nbody_word2vec_train = train_data['body_processed'].apply(generate_word_vectors)\n\n# Compute cosine similarity between Word2Vec features of headline and body for train dataset\ncosine_sim_word2vec_train = []\nfor h_w2v, b_w2v in zip(headline_word2vec_train, body_word2vec_train):\n    cosine_sim_word2vec_train.append(cosine_similarity(h_w2v.reshape(1, -1), b_w2v.reshape(1, -1))[0][0])\n\n# Create DataFrame for Word2Vec features for train dataset\nword2vec_features_train = pd.DataFrame({\n    'cosine_similarity_word2vec': cosine_sim_word2vec_train\n})\n\n# Generate word vectors for headline and body for train dataset\nheadline_word2vec_valid = valid_data['headline_processed'].apply(generate_word_vectors)\nbody_word2vec_valid = valid_data['body_processed'].apply(generate_word_vectors)\n\n# Compute cosine similarity between Word2Vec features of headline and body for train dataset\ncosine_sim_word2vec_valid = []\nfor h_w2v, b_w2v in zip(headline_word2vec_valid, body_word2vec_valid):\n    cosine_sim_word2vec_valid.append(cosine_similarity(h_w2v.reshape(1, -1), b_w2v.reshape(1, -1))[0][0])\n\n# Create DataFrame for Word2Vec features for train dataset\nword2vec_features_valid = pd.DataFrame({\n    'cosine_similarity_word2vec': cosine_sim_word2vec_valid\n})\n\n# Generate word vectors for headline and body for test dataset\nheadline_word2vec_test = test_data['headline_processed'].apply(generate_word_vectors)\nbody_word2vec_test = test_data['body_processed'].apply(generate_word_vectors)\n\n# Compute cosine similarity between Word2Vec features of headline and body for test dataset\ncosine_sim_word2vec_test = []\nfor h_w2v, b_w2v in zip(headline_word2vec_test, body_word2vec_test):\n    cosine_sim_word2vec_test.append(cosine_similarity(h_w2v.reshape(1, -1), b_w2v.reshape(1, -1))[0][0])\n\n# Create DataFrame for Word2Vec features for test dataset\nword2vec_features_test = pd.DataFrame({\n    'cosine_similarity_word2vec': cosine_sim_word2vec_test\n})","metadata":{"execution":{"iopub.status.busy":"2024-04-15T05:15:46.561501Z","iopub.execute_input":"2024-04-15T05:15:46.562328Z","iopub.status.idle":"2024-04-15T05:17:12.829236Z","shell.execute_reply.started":"2024-04-15T05:15:46.562293Z","shell.execute_reply":"2024-04-15T05:17:12.828226Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from nltk.sentiment import SentimentIntensityAnalyzer\n\n# Function to calculate sentiment polarity scores for train dataset\ndef calculate_sentiment_train(text):\n    sid = SentimentIntensityAnalyzer()\n    sentiment_scores = sid.polarity_scores(text)\n    return sentiment_scores['compound']  # Using compound score as it combines positive, negative, and neutral scores\n\n# Calculate sentiment polarity scores for headline and body for train dataset\nheadline_sentiment_train = train_data['headline_processed'].apply(calculate_sentiment_train)\nbody_sentiment_train = train_data['body_processed'].apply(calculate_sentiment_train)\n\n# Create DataFrame for sentiment features for train dataset\nsentiment_features_train = pd.DataFrame({\n    'headline_sentiment': headline_sentiment_train,\n    'body_sentiment': body_sentiment_train\n})\n\ndef calculate_sentiment_valid(text):\n    sid = SentimentIntensityAnalyzer()\n    sentiment_scores = sid.polarity_scores(text)\n    return sentiment_scores['compound']  # Using compound score as it combines positive, negative, and neutral scores\n\n# Calculate sentiment polarity scores for headline and body for train dataset\nheadline_sentiment_valid = valid_data['headline_processed'].apply(calculate_sentiment_valid)\nbody_sentiment_valid = valid_data['body_processed'].apply(calculate_sentiment_valid)\n\n# Create DataFrame for sentiment features for train dataset\nsentiment_features_valid = pd.DataFrame({\n    'headline_sentiment': headline_sentiment_valid,\n    'body_sentiment': body_sentiment_valid\n})\n\n# Function to calculate sentiment polarity scores for test dataset\ndef calculate_sentiment_test(text):\n    sid = SentimentIntensityAnalyzer()\n    sentiment_scores = sid.polarity_scores(text)\n    return sentiment_scores['compound']  # Using compound score as it combines positive, negative, and neutral scores\n\n# Calculate sentiment polarity scores for headline and body for test dataset\nheadline_sentiment_test = test_data['headline_processed'].apply(calculate_sentiment_test)\nbody_sentiment_test = test_data['body_processed'].apply(calculate_sentiment_test)\n\n# Create DataFrame for sentiment features for test dataset\nsentiment_features_test = pd.DataFrame({\n    'headline_sentiment': headline_sentiment_test,\n    'body_sentiment': body_sentiment_test\n})","metadata":{"execution":{"iopub.status.busy":"2024-04-15T05:17:51.082143Z","iopub.execute_input":"2024-04-15T05:17:51.082740Z","iopub.status.idle":"2024-04-15T05:27:15.836303Z","shell.execute_reply.started":"2024-04-15T05:17:51.082708Z","shell.execute_reply":"2024-04-15T05:27:15.835454Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n# Concatenate all the generated features for train dataset\nall_features_train = pd.concat([count_features_train, tfidf_features_train, svd_features_train, word2vec_features_train, sentiment_features_train], axis=1)\n\n# Define features and labels for train dataset\nX_train = all_features_train\ny_train = train_data['stance_cat']\n\n# Construct LightGBM classifier\nbase_model = lgb.LGBMClassifier()\nfinal_model = GaussianNB()\n\nclf = StackingClassifier(\n    estimators=[('lgb', base_model)], \n    final_estimator=final_model,\n    stack_method='predict_proba', \n    passthrough=False  \n)\n\n# Fit classifier on train dataset\nclf.fit(X_train, y_train)\n\n# Perform cross-validation on train dataset\nscores_train = cross_val_score(clf, X_train, y_train, cv=10)\nprint(\"Accuracy for Train Dataset:\", np.mean(scores_train))\n\n# Concatenate all the generated features for validation dataset\nall_features_valid = pd.concat([count_features_valid, tfidf_features_valid, svd_features_valid, word2vec_features_valid, sentiment_features_valid], axis=1)\n\n# Define features and labels for validation dataset\nX_valid = all_features_valid\ny_valid = valid_data['stance_cat']\n\n# Predict on validation dataset\ny_pred_valid = clf.predict(X_valid)\n\n# Calculate accuracy on validation dataset\naccuracy_valid = accuracy_score(y_valid, y_pred_valid)\nprint(\"Accuracy for Validation Dataset:\", accuracy_valid)\n\n# Concatenate all the generated features for test dataset\nall_features_test = pd.concat([count_features_test, tfidf_features_test, svd_features_test, word2vec_features_test, sentiment_features_test], axis=1)\n\n# Define features and labels for test dataset\nX_test = all_features_test\ny_test = test_data['stance_cat']\n\n# Predict on test dataset\ny_pred_test = clf.predict(X_test)\n\n# Calculate accuracy on test dataset\naccuracy_test = accuracy_score(y_test, y_pred_test)\nprint(\"Accuracy for Test Dataset:\", accuracy_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T05:27:15.838313Z","iopub.execute_input":"2024-04-15T05:27:15.838598Z","iopub.status.idle":"2024-04-15T05:28:11.305359Z","shell.execute_reply.started":"2024-04-15T05:27:15.838574Z","shell.execute_reply":"2024-04-15T05:28:11.304223Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002019 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1963\n[LightGBM] [Info] Number of data points in the train set: 21483, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988172\n[LightGBM] [Info] Start training from score -3.464759\n[LightGBM] [Info] Start training from score -1.103372\n[LightGBM] [Info] Start training from score -0.693101\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001584 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1948\n[LightGBM] [Info] Number of data points in the train set: 17186, number of used features: 10\n[LightGBM] [Info] Start training from score -1.987979\n[LightGBM] [Info] Start training from score -3.465852\n[LightGBM] [Info] Start training from score -1.103278\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001565 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1954\n[LightGBM] [Info] Number of data points in the train set: 17186, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988404\n[LightGBM] [Info] Start training from score -3.463992\n[LightGBM] [Info] Start training from score -1.103278\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001718 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1947\n[LightGBM] [Info] Number of data points in the train set: 17186, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988404\n[LightGBM] [Info] Start training from score -3.463992\n[LightGBM] [Info] Start training from score -1.103454\n[LightGBM] [Info] Start training from score -0.693031\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001565 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1946\n[LightGBM] [Info] Number of data points in the train set: 17187, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988037\n[LightGBM] [Info] Start training from score -3.464050\n[LightGBM] [Info] Start training from score -1.103512\n[LightGBM] [Info] Start training from score -0.693089\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001588 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1944\n[LightGBM] [Info] Number of data points in the train set: 17187, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988037\n[LightGBM] [Info] Start training from score -3.465910\n[LightGBM] [Info] Start training from score -1.103336\n[LightGBM] [Info] Start training from score -0.693089\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001775 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1957\n[LightGBM] [Info] Number of data points in the train set: 19334, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988061\n[LightGBM] [Info] Start training from score -3.466046\n[LightGBM] [Info] Start training from score -1.103226\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001462 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1943\n[LightGBM] [Info] Number of data points in the train set: 15467, number of used features: 10\n[LightGBM] [Info] Start training from score -1.987764\n[LightGBM] [Info] Start training from score -3.466447\n[LightGBM] [Info] Start training from score -1.103213\n[LightGBM] [Info] Start training from score -0.693212\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001426 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1944\n[LightGBM] [Info] Number of data points in the train set: 15467, number of used features: 10\n[LightGBM] [Info] Start training from score -1.987764\n[LightGBM] [Info] Start training from score -3.466447\n[LightGBM] [Info] Start training from score -1.103213\n[LightGBM] [Info] Start training from score -0.693212\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001418 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1944\n[LightGBM] [Info] Number of data points in the train set: 15467, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988236\n[LightGBM] [Info] Start training from score -3.466447\n[LightGBM] [Info] Start training from score -1.103213\n[LightGBM] [Info] Start training from score -0.693083\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001382 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1943\n[LightGBM] [Info] Number of data points in the train set: 15467, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988236\n[LightGBM] [Info] Start training from score -3.466447\n[LightGBM] [Info] Start training from score -1.103213\n[LightGBM] [Info] Start training from score -0.693083\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001427 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1937\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103278\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001875 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1956\n[LightGBM] [Info] Number of data points in the train set: 19334, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988438\n[LightGBM] [Info] Start training from score -3.464392\n[LightGBM] [Info] Start training from score -1.103226\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001371 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1943\n[LightGBM] [Info] Number of data points in the train set: 15467, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988236\n[LightGBM] [Info] Start training from score -3.464379\n[LightGBM] [Info] Start training from score -1.103213\n[LightGBM] [Info] Start training from score -0.693212\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001399 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1939\n[LightGBM] [Info] Number of data points in the train set: 15467, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988236\n[LightGBM] [Info] Start training from score -3.464379\n[LightGBM] [Info] Start training from score -1.103213\n[LightGBM] [Info] Start training from score -0.693212\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001373 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1934\n[LightGBM] [Info] Number of data points in the train set: 15467, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988709\n[LightGBM] [Info] Start training from score -3.464379\n[LightGBM] [Info] Start training from score -1.103213\n[LightGBM] [Info] Start training from score -0.693083\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001372 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1934\n[LightGBM] [Info] Number of data points in the train set: 15467, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988709\n[LightGBM] [Info] Start training from score -3.464379\n[LightGBM] [Info] Start training from score -1.103213\n[LightGBM] [Info] Start training from score -0.693083\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001446 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1934\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103278\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001856 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1960\n[LightGBM] [Info] Number of data points in the train set: 19334, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988438\n[LightGBM] [Info] Start training from score -3.464392\n[LightGBM] [Info] Start training from score -1.103382\n[LightGBM] [Info] Start training from score -0.693044\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001395 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1945\n[LightGBM] [Info] Number of data points in the train set: 15467, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988236\n[LightGBM] [Info] Start training from score -3.464379\n[LightGBM] [Info] Start training from score -1.103408\n[LightGBM] [Info] Start training from score -0.693083\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001380 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1946\n[LightGBM] [Info] Number of data points in the train set: 15467, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988236\n[LightGBM] [Info] Start training from score -3.464379\n[LightGBM] [Info] Start training from score -1.103408\n[LightGBM] [Info] Start training from score -0.693083\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001375 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1945\n[LightGBM] [Info] Number of data points in the train set: 15467, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988709\n[LightGBM] [Info] Start training from score -3.464379\n[LightGBM] [Info] Start training from score -1.103213\n[LightGBM] [Info] Start training from score -0.693083\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001365 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1939\n[LightGBM] [Info] Number of data points in the train set: 15467, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988709\n[LightGBM] [Info] Start training from score -3.464379\n[LightGBM] [Info] Start training from score -1.103408\n[LightGBM] [Info] Start training from score -0.692953\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001486 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1940\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693018\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001734 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1960\n[LightGBM] [Info] Number of data points in the train set: 19335, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988112\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103434\n[LightGBM] [Info] Start training from score -0.693095\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001408 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1941\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.987829\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001826 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1946\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.987829\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001370 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1942\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103278\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001473 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1942\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693018\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001425 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1936\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693018\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001755 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1957\n[LightGBM] [Info] Number of data points in the train set: 19335, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988112\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103434\n[LightGBM] [Info] Start training from score -0.693095\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001478 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1943\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.987829\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001500 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1947\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.987829\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001504 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1942\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103278\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001472 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1937\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693018\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001382 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1939\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693018\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001727 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1954\n[LightGBM] [Info] Number of data points in the train set: 19335, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988112\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103434\n[LightGBM] [Info] Start training from score -0.693095\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002391 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1941\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.987829\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001589 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1945\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.987829\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001410 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1942\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103278\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001410 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1939\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693018\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001380 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1939\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693018\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001716 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1955\n[LightGBM] [Info] Number of data points in the train set: 19335, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988112\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103434\n[LightGBM] [Info] Start training from score -0.693095\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001440 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1938\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.987829\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001371 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1943\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.987829\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001398 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1939\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103278\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001371 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1937\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693018\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001390 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1935\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693018\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001939 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1954\n[LightGBM] [Info] Number of data points in the train set: 19335, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988112\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103434\n[LightGBM] [Info] Start training from score -0.693095\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001426 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1942\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.987829\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001380 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1943\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.987829\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001384 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1937\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103278\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001400 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1937\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693018\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001371 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1934\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693018\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001740 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1954\n[LightGBM] [Info] Number of data points in the train set: 19335, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988112\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103434\n[LightGBM] [Info] Start training from score -0.693095\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001383 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1938\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.987829\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001391 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1942\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.987829\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001411 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1941\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103278\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001461 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1934\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693018\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001464 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1933\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103473\n[LightGBM] [Info] Start training from score -0.693018\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001830 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1955\n[LightGBM] [Info] Number of data points in the train set: 19335, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988112\n[LightGBM] [Info] Start training from score -3.466098\n[LightGBM] [Info] Start training from score -1.103278\n[LightGBM] [Info] Start training from score -0.693095\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001372 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1936\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.987829\n[LightGBM] [Info] Start training from score -3.466512\n[LightGBM] [Info] Start training from score -1.103278\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001418 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1945\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.987829\n[LightGBM] [Info] Start training from score -3.466512\n[LightGBM] [Info] Start training from score -1.103278\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001433 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1941\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.464444\n[LightGBM] [Info] Start training from score -1.103278\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001847 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1934\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.466512\n[LightGBM] [Info] Start training from score -1.103278\n[LightGBM] [Info] Start training from score -0.693018\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001706 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1933\n[LightGBM] [Info] Number of data points in the train set: 15468, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988301\n[LightGBM] [Info] Start training from score -3.466512\n[LightGBM] [Info] Start training from score -1.103278\n[LightGBM] [Info] Start training from score -0.693018\nAccuracy for Train Dataset: 0.8384295930808406\nAccuracy for Validation Dataset: 0.8507073715562175\nAccuracy for Test Dataset: 0.8249534450651769\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\n\n# Train the LightGBoost classifier on train dataset\nclf.fit(X_train, y_train)\n\n# Make predictions on train dataset\ny_train_pred = clf.predict(X_train)\n\n# Calculate precision, recall, and F1-score for each class on train dataset\nprecision_train = precision_score(y_train, y_train_pred, average=None)\nrecall_train = recall_score(y_train, y_train_pred, average=None)\nf1_train = f1_score(y_train, y_train_pred, average=None)\n\n# Calculate macro-averaged precision, recall, and F1-score on train dataset\nmacro_precision_train = precision_score(y_train, y_train_pred, average='macro')\nmacro_recall_train = recall_score(y_train, y_train_pred, average='macro')\nmacro_f1_train = f1_score(y_train, y_train_pred, average='macro')\n\n# Print the results for train dataset\nprint(\"Train Dataset:\")\nprint(\"LightGBoost Accuracy:\", np.mean(scores_train))\nprint(\"Class 0 - Precision:\", precision_train[0], \", Recall:\", recall_train[0], \", F1-score:\", f1_train[0])\nprint(\"Class 1 - Precision:\", precision_train[1], \", Recall:\", recall_train[1], \", F1-score:\", f1_train[1])\nprint(\"Class 2 - Precision:\", precision_train[2], \", Recall:\", recall_train[2], \", F1-score:\", f1_train[2])\nprint(\"Class 3 - Precision:\", precision_train[3], \", Recall:\", recall_train[3], \", F1-score:\", f1_train[3])\nprint(\"Macro Precision:\", macro_precision_train)\nprint(\"Macro Recall:\", macro_recall_train)\nprint(\"Macro F1 Score:\", macro_f1_train)\n\n# Make predictions on test dataset\ny_valid_pred = clf.predict(X_valid)\n\n# Calculate precision, recall, and F1-score for each class on test dataset\nprecision_valid = precision_score(y_valid, y_valid_pred, average=None)\nrecall_valid = recall_score(y_valid, y_valid_pred, average=None)\nf1_valid = f1_score(y_valid, y_valid_pred, average=None)\n\n# Calculate macro-averaged precision, recall, and F1-score on test dataset\nmacro_precision_valid = precision_score(y_valid, y_valid_pred, average='macro')\nmacro_recall_valid = recall_score(y_valid, y_valid_pred, average='macro')\nmacro_f1_valid = f1_score(y_valid, y_valid_pred, average='macro')\n\n# Print the results for test dataset\nprint(\"\\nValidation Dataset:\")\nprint(\"Accuracy for Validation Dataset:\", accuracy_valid)\nprint(\"Class 0 - Precision:\", precision_valid[0], \", Recall:\", recall_valid[0], \", F1-score:\", f1_valid[0])\nprint(\"Class 1 - Precision:\", precision_valid[1], \", Recall:\", recall_valid[1], \", F1-score:\", f1_valid[1])\nprint(\"Class 2 - Precision:\", precision_valid[2], \", Recall:\", recall_valid[2], \", F1-score:\", f1_valid[2])\nprint(\"Class 3 - Precision:\", precision_valid[3], \", Recall:\", recall_valid[3], \", F1-score:\", f1_valid[3])\nprint(\"Macro Precision:\", macro_precision_valid)\nprint(\"Macro Recall:\", macro_recall_valid)\nprint(\"Macro F1 Score:\", macro_f1_valid)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T05:28:11.306922Z","iopub.execute_input":"2024-04-15T05:28:11.307220Z","iopub.status.idle":"2024-04-15T05:28:17.008033Z","shell.execute_reply.started":"2024-04-15T05:28:11.307195Z","shell.execute_reply":"2024-04-15T05:28:17.007094Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002664 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1963\n[LightGBM] [Info] Number of data points in the train set: 21483, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988172\n[LightGBM] [Info] Start training from score -3.464759\n[LightGBM] [Info] Start training from score -1.103372\n[LightGBM] [Info] Start training from score -0.693101\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001590 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1948\n[LightGBM] [Info] Number of data points in the train set: 17186, number of used features: 10\n[LightGBM] [Info] Start training from score -1.987979\n[LightGBM] [Info] Start training from score -3.465852\n[LightGBM] [Info] Start training from score -1.103278\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001677 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1954\n[LightGBM] [Info] Number of data points in the train set: 17186, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988404\n[LightGBM] [Info] Start training from score -3.463992\n[LightGBM] [Info] Start training from score -1.103278\n[LightGBM] [Info] Start training from score -0.693147\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001789 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1947\n[LightGBM] [Info] Number of data points in the train set: 17186, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988404\n[LightGBM] [Info] Start training from score -3.463992\n[LightGBM] [Info] Start training from score -1.103454\n[LightGBM] [Info] Start training from score -0.693031\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001646 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1946\n[LightGBM] [Info] Number of data points in the train set: 17187, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988037\n[LightGBM] [Info] Start training from score -3.464050\n[LightGBM] [Info] Start training from score -1.103512\n[LightGBM] [Info] Start training from score -0.693089\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001569 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1944\n[LightGBM] [Info] Number of data points in the train set: 17187, number of used features: 10\n[LightGBM] [Info] Start training from score -1.988037\n[LightGBM] [Info] Start training from score -3.465910\n[LightGBM] [Info] Start training from score -1.103336\n[LightGBM] [Info] Start training from score -0.693089\nTrain Dataset:\nLightGBoost Accuracy: 0.8384295930808406\nClass 0 - Precision: 0.8668578451454322 , Recall: 0.7192386131883073 , F1-score: 0.786178710756084\nClass 1 - Precision: 0.8801213960546282 , Recall: 0.8630952380952381 , F1-score: 0.8715251690458303\nClass 2 - Precision: 0.8641611082606465 , Recall: 0.9452785183106497 , F1-score: 0.9029015613482544\nClass 3 - Precision: 0.9875318787191839 , Recall: 0.9732824427480916 , F1-score: 0.9803553846875146\nMacro Precision: 0.8996680570449727\nMacro Recall: 0.8752237030855717\nMacro F1 Score: 0.8852402064594208\n\nValidation Dataset:\nAccuracy for Validation Dataset: 0.8507073715562175\nClass 0 - Precision: 0.6630824372759857 , Recall: 0.5027173913043478 , F1-score: 0.5718701700154559\nClass 1 - Precision: 0.45454545454545453 , Recall: 0.23809523809523808 , F1-score: 0.31249999999999994\nClass 2 - Precision: 0.7669902912621359 , Recall: 0.8866442199775533 , F1-score: 0.8224882873503384\nClass 3 - Precision: 0.967741935483871 , Recall: 0.960536113179449 , F1-score: 0.9641255605381166\nMacro Precision: 0.7130900296418617\nMacro Recall: 0.646998240639147\nMacro F1 Score: 0.6677460044759777\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support\ndef score_submission(gold_labels, test_labels):\n    score = 0.0\n\n    for i, (g, t) in enumerate(zip(gold_labels, test_labels)):\n        g_stance, t_stance = g, t\n        if g_stance == t_stance:\n            score += 0.25\n            if g_stance != 'unrelated':\n                score += 0.50\n        if g_stance in [0, 1, 2] and t_stance in [0, 1, 2]:\n            score += 0.25\n\n    return score\n\ndef report_score(actual,predicted):\n    score = score_submission(actual,predicted)\n    best_score = score_submission(actual,actual)\n\n    print(\"Score: \" +str(score) + \" out of \" + str(best_score) + \"\\t(\"+str(score*100/best_score) + \"%)\")\n    return score*100/best_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on test dataset\ny_test_pred = clf.predict(X_test)\n\n# Calculate precision, recall, and F1-score for each class on test dataset\nprecision_test = precision_score(y_test, y_test_pred, average=None)\nrecall_test = recall_score(y_test, y_test_pred, average=None)\nf1_test = f1_score(y_test, y_test_pred, average=None)\n\n# Calculate macro-averaged precision, recall, and F1-score on test dataset\nmacro_precision_test = precision_score(y_test, y_test_pred, average='macro')\nmacro_recall_test = recall_score(y_test, y_test_pred, average='macro')\nmacro_f1_test = f1_score(y_test, y_test_pred, average='macro')\n\n# Print the results for test dataset\nprint(\"\\nTest Dataset:\")\nprint(\"Accuracy for Test Dataset:\", accuracy_test)\nprint(\"Class 0 - Precision:\", precision_test[0], \", Recall:\", recall_test[0], \", F1-score:\", f1_test[0])\nprint(\"Class 1 - Precision:\", precision_test[1], \", Recall:\", recall_test[1], \", F1-score:\", f1_test[1])\nprint(\"Class 2 - Precision:\", precision_test[2], \", Recall:\", recall_test[2], \", F1-score:\", f1_test[2])\nprint(\"Class 3 - Precision:\", precision_test[3], \", Recall:\", recall_test[3], \", F1-score:\", f1_test[3])\nprint(\"Macro Precision:\", macro_precision_test)\nprint(\"Macro Recall:\", macro_recall_test)\nprint(\"Macro F1 Score:\", macro_f1_test)\n\ncustom_score = score_submission(y_test, y_test_pred)\nreport_sc = report_score(y_test, y_test_pred)\n\nprint(\"Custom Score:\", custom_score)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T05:54:07.171616Z","iopub.execute_input":"2024-04-15T05:54:07.172013Z","iopub.status.idle":"2024-04-15T05:54:07.246824Z","shell.execute_reply.started":"2024-04-15T05:54:07.171987Z","shell.execute_reply":"2024-04-15T05:54:07.246014Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"\nTest Dataset:\nAccuracy for Test Dataset: 0.8249534450651769\nClass 0 - Precision: 0.576271186440678 , Recall: 0.46195652173913043 , F1-score: 0.5128205128205129\nClass 1 - Precision: 0.3392857142857143 , Recall: 0.2261904761904762 , F1-score: 0.27142857142857146\nClass 2 - Precision: 0.7446601941747573 , Recall: 0.8608305274971941 , F1-score: 0.7985424258198854\nClass 3 - Precision: 0.9654907975460123 , Recall: 0.9381520119225037 , F1-score: 0.9516250944822373\nMacro Precision: 0.6564269731117904\nMacro Recall: 0.6217823843373261\nMacro F1 Score: 0.6336041511378018\nScore: 1985.75 out of 2349.5\t(84.51798254947862%)\nCustom Score: 1985.75\n","output_type":"stream"}]}]}