{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8070813,"sourceType":"datasetVersion","datasetId":4762146},{"sourceId":8081859,"sourceType":"datasetVersion","datasetId":4770247},{"sourceId":8084345,"sourceType":"datasetVersion","datasetId":4772088},{"sourceId":8092275,"sourceType":"datasetVersion","datasetId":4777705}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom gensim.models import KeyedVectors\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.naive_bayes import GaussianNB  # Import Naive Bayes\nfrom sklearn.ensemble import StackingClassifier\nmodel_selected = \"xgb\"\n\n# Load the dataset\ntrain_data = pd.read_csv('/kaggle/input/lemma-balanced/lemmatized_dataset_final_balanced_train.csv')\nvalid_data = pd.read_csv('/kaggle/input/lemma-balanced/lemmatized_dataset_final_balanced_validation.csv')\ntest_data = pd.read_csv('/kaggle/input/lemma-balanced/lemmatized_dataset_final_balanced_test.csv')\n\n# Display the first few rows of the dataset\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:03:53.552662Z","iopub.execute_input":"2024-04-15T06:03:53.552939Z","iopub.status.idle":"2024-04-15T06:04:11.514946Z","shell.execute_reply.started":"2024-04-15T06:03:53.552915Z","shell.execute_reply":"2024-04-15T06:04:11.513954Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n  warnings.warn(\"The twython library has not been installed. \"\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"                                            Headline  Body ID     Stance  \\\n0  dna test confirm lebanon is holding isi leader...     2042  unrelated   \n1  somalia shebab chief ahmed abdi godane likely ...     1610    discuss   \n2  dna test prove lebanon is holding isi chief al...     1468   disagree   \n3  the pumpkinspice condom is just a figment of y...     1253  unrelated   \n4  u probing claim isi fighter seized airdropped ...      465    discuss   \n\n                                         articleBody  stance_cat  \n0  there is a story currently making the round ab...           3  \n1  ahmed abdi godane the leader of al shabab the ...           2  \n2  an iraqi official denied that a woman detained...           1  \n3  the united state department of defense said on...           3  \n4  the pentagon admitted on wednesday that isi di...           2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Headline</th>\n      <th>Body ID</th>\n      <th>Stance</th>\n      <th>articleBody</th>\n      <th>stance_cat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>dna test confirm lebanon is holding isi leader...</td>\n      <td>2042</td>\n      <td>unrelated</td>\n      <td>there is a story currently making the round ab...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>somalia shebab chief ahmed abdi godane likely ...</td>\n      <td>1610</td>\n      <td>discuss</td>\n      <td>ahmed abdi godane the leader of al shabab the ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>dna test prove lebanon is holding isi chief al...</td>\n      <td>1468</td>\n      <td>disagree</td>\n      <td>an iraqi official denied that a woman detained...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>the pumpkinspice condom is just a figment of y...</td>\n      <td>1253</td>\n      <td>unrelated</td>\n      <td>the united state department of defense said on...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>u probing claim isi fighter seized airdropped ...</td>\n      <td>465</td>\n      <td>discuss</td>\n      <td>the pentagon admitted on wednesday that isi di...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"valid_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:04:11.516731Z","iopub.execute_input":"2024-04-15T06:04:11.517038Z","iopub.status.idle":"2024-04-15T06:04:11.527343Z","shell.execute_reply.started":"2024-04-15T06:04:11.517012Z","shell.execute_reply":"2024-04-15T06:04:11.526433Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                            Headline  Body ID     Stance  \\\n0  james foley american journalist james wright f...     1336      agree   \n1  yous hostage luke somers killed during yemen r...     1119    discuss   \n2  the internet tried to make axl rose it latest ...     1857  unrelated   \n3  conde nast rat problem at one world trade cent...      821      agree   \n4  video marine survives taliban sniper headshot ...     1256  unrelated   \n\n                                         articleBody  stance_cat  \n0  two yous official say they believe american jo...           0  \n1  a britishborn photographer held hostage by al ...           2  \n2  those caught with the same moniker a the dicta...           3  \n3  the rat infestation in vogue new 1 world trade...           0  \n4  the pentagon ha confirmed that ahmed abdi goda...           3  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Headline</th>\n      <th>Body ID</th>\n      <th>Stance</th>\n      <th>articleBody</th>\n      <th>stance_cat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>james foley american journalist james wright f...</td>\n      <td>1336</td>\n      <td>agree</td>\n      <td>two yous official say they believe american jo...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>yous hostage luke somers killed during yemen r...</td>\n      <td>1119</td>\n      <td>discuss</td>\n      <td>a britishborn photographer held hostage by al ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>the internet tried to make axl rose it latest ...</td>\n      <td>1857</td>\n      <td>unrelated</td>\n      <td>those caught with the same moniker a the dicta...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>conde nast rat problem at one world trade cent...</td>\n      <td>821</td>\n      <td>agree</td>\n      <td>the rat infestation in vogue new 1 world trade...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>video marine survives taliban sniper headshot ...</td>\n      <td>1256</td>\n      <td>unrelated</td>\n      <td>the pentagon ha confirmed that ahmed abdi goda...</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:04:11.528467Z","iopub.execute_input":"2024-04-15T06:04:11.528792Z","iopub.status.idle":"2024-04-15T06:04:11.542297Z","shell.execute_reply.started":"2024-04-15T06:04:11.528758Z","shell.execute_reply":"2024-04-15T06:04:11.541358Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                            Headline  Body ID     Stance  \\\n0  god is a woman priest who died for 48 minute c...     1227   disagree   \n1  update internet report of ebola outbreak in pu...      958  unrelated   \n2  british hostage david haines beheaded by islam...     2034  unrelated   \n3  video messaging service verifies timing of cnn...     1435  unrelated   \n4  omar gonzalez white house fencejumper made it ...      917  unrelated   \n\n                                         articleBody  stance_cat  \n0  a supposed catholic priest claim of seeing god...           1  \n1  an animal lover from norfolk splashed hundred ...           3  \n2  when a report went viral that nbc meteorologis...           3  \n3  apple may be planning to hold a special event ...           3  \n4  source detroit free press yesterday we reporte...           3  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Headline</th>\n      <th>Body ID</th>\n      <th>Stance</th>\n      <th>articleBody</th>\n      <th>stance_cat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>god is a woman priest who died for 48 minute c...</td>\n      <td>1227</td>\n      <td>disagree</td>\n      <td>a supposed catholic priest claim of seeing god...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>update internet report of ebola outbreak in pu...</td>\n      <td>958</td>\n      <td>unrelated</td>\n      <td>an animal lover from norfolk splashed hundred ...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>british hostage david haines beheaded by islam...</td>\n      <td>2034</td>\n      <td>unrelated</td>\n      <td>when a report went viral that nbc meteorologis...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>video messaging service verifies timing of cnn...</td>\n      <td>1435</td>\n      <td>unrelated</td>\n      <td>apple may be planning to hold a special event ...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>omar gonzalez white house fencejumper made it ...</td>\n      <td>917</td>\n      <td>unrelated</td>\n      <td>source detroit free press yesterday we reporte...</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_data['stance_cat'] = train_data['Stance'].map({'agree':0,'disagree':1,'discuss':2,'unrelated':3}).astype(int)\ntrain_data['Stance'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:04:11.544796Z","iopub.execute_input":"2024-04-15T06:04:11.545402Z","iopub.status.idle":"2024-04-15T06:04:11.568669Z","shell.execute_reply.started":"2024-04-15T06:04:11.545370Z","shell.execute_reply":"2024-04-15T06:04:11.567932Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Stance\nunrelated    10742\ndiscuss       7127\nagree         2942\ndisagree       672\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"valid_data['stance_cat'] = valid_data['Stance'].map({'agree':0,'disagree':1,'discuss':2,'unrelated':3}).astype(int)\nvalid_data['Stance'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:04:11.569511Z","iopub.execute_input":"2024-04-15T06:04:11.569730Z","iopub.status.idle":"2024-04-15T06:04:11.579110Z","shell.execute_reply.started":"2024-04-15T06:04:11.569711Z","shell.execute_reply":"2024-04-15T06:04:11.578285Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Stance\nunrelated    1343\ndiscuss       891\nagree         368\ndisagree       84\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"test_data['stance_cat'] = test_data['Stance'].map({'agree':0,'disagree':1,'discuss':2,'unrelated':3}).astype(int)\ntest_data['Stance'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:04:11.580042Z","iopub.execute_input":"2024-04-15T06:04:11.580344Z","iopub.status.idle":"2024-04-15T06:04:11.591003Z","shell.execute_reply.started":"2024-04-15T06:04:11.580314Z","shell.execute_reply":"2024-04-15T06:04:11.590024Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Stance\nunrelated    1342\ndiscuss       891\nagree         368\ndisagree       84\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"import torch\n\n# Check if CUDA (GPU support) is available\nif torch.cuda.is_available():\n    print('CUDA is available. GPU will be used.')\nelse:\n    print('CUDA is not available. CPU will be used.')","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:04:11.592522Z","iopub.execute_input":"2024-04-15T06:04:11.592802Z","iopub.status.idle":"2024-04-15T06:04:15.788254Z","shell.execute_reply.started":"2024-04-15T06:04:11.592779Z","shell.execute_reply":"2024-04-15T06:04:15.787279Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"CUDA is available. GPU will be used.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Updated preprocess_text function\ndef preprocess_text(text):\n    # Tokenize text\n    tokens = word_tokenize(text)\n    return \" \".join(tokens)\n\n# Applying the updated preprocess_text function\ntrain_data['headline_processed'] = train_data['Headline'].apply(preprocess_text)\ntrain_data['body_processed'] = train_data['articleBody'].apply(preprocess_text)\n\nvalid_data['headline_processed'] = valid_data['Headline'].apply(preprocess_text)\nvalid_data['body_processed'] = valid_data['articleBody'].apply(preprocess_text)\n\ntest_data['headline_processed'] = test_data['Headline'].apply(preprocess_text)\ntest_data['body_processed'] = test_data['articleBody'].apply(preprocess_text)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:04:15.789459Z","iopub.execute_input":"2024-04-15T06:04:15.790079Z","iopub.status.idle":"2024-04-15T06:05:36.033635Z","shell.execute_reply.started":"2024-04-15T06:04:15.790055Z","shell.execute_reply":"2024-04-15T06:05:36.032769Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Count how many times each gram appears in the headline and body\nheadline_unigrams = train_data['headline_processed'].apply(lambda x: x.split())\nbody_unigrams = train_data['body_processed'].apply(lambda x: x.split())\n\n# Count the number of unique grams in the headline and body\nheadline_unique_counts = headline_unigrams.apply(lambda x: len(set(x)))\nbody_unique_counts = body_unigrams.apply(lambda x: len(set(x)))\n\n# Compute the ratio between the count of grams in the headline and body\nratio_counts = headline_unique_counts / body_unique_counts\n\n# Compute overlapping count of grams between headline and body\noverlapping_counts = []\nfor headline, body in zip(headline_unigrams, body_unigrams):\n    overlapping_counts.append(len(set(headline) & set(body)))\n\n# Normalize overlapping count by the number of grams in the headline\nnormalized_overlapping_counts = [oc / len(h) if len(h) > 0 else 0 for oc, h in zip(overlapping_counts, headline_unigrams)]\n\n# Create DataFrame for count features\ncount_features_train = pd.DataFrame({\n    'headline_unique_counts': headline_unique_counts,\n    'body_unique_counts': body_unique_counts,\n    'ratio_counts': ratio_counts,\n    'overlapping_counts': overlapping_counts,\n    'normalized_overlapping_counts': normalized_overlapping_counts\n})\n\n#Validation set \n# Count how many times each gram appears in the headline and body\nheadline_unigrams = valid_data['headline_processed'].apply(lambda x: x.split())\nbody_unigrams = valid_data['body_processed'].apply(lambda x: x.split())\n\n# Count the number of unique grams in the headline and body\nheadline_unique_counts = headline_unigrams.apply(lambda x: len(set(x)))\nbody_unique_counts = body_unigrams.apply(lambda x: len(set(x)))\n\n# Compute the ratio between the count of grams in the headline and body\nratio_counts = headline_unique_counts / body_unique_counts\n\n# Compute overlapping count of grams between headline and body\noverlapping_counts = []\nfor headline, body in zip(headline_unigrams, body_unigrams):\n    overlapping_counts.append(len(set(headline) & set(body)))\n\n# Normalize overlapping count by the number of grams in the headline\nnormalized_overlapping_counts = [oc / len(h) if len(h) > 0 else 0 for oc, h in zip(overlapping_counts, headline_unigrams)]\n\n# Create DataFrame for count features\ncount_features_valid = pd.DataFrame({\n    'headline_unique_counts': headline_unique_counts,\n    'body_unique_counts': body_unique_counts,\n    'ratio_counts': ratio_counts,\n    'overlapping_counts': overlapping_counts,\n    'normalized_overlapping_counts': normalized_overlapping_counts\n})\n\n# Applying the same process for test data\n# Count how many times each gram appears in the headline and body\ntest_headline_unigrams = test_data['headline_processed'].apply(lambda x: x.split())\ntest_body_unigrams = test_data['body_processed'].apply(lambda x: x.split())\n\n# Count the number of unique grams in the headline and body\ntest_headline_unique_counts = test_headline_unigrams.apply(lambda x: len(set(x)))\ntest_body_unique_counts = test_body_unigrams.apply(lambda x: len(set(x)))\n\n# Compute the ratio between the count of grams in the headline and body\ntest_ratio_counts = test_headline_unique_counts / test_body_unique_counts\n\n# Compute overlapping count of grams between headline and body\ntest_overlapping_counts = []\nfor headline, body in zip(test_headline_unigrams, test_body_unigrams):\n    test_overlapping_counts.append(len(set(headline) & set(body)))\n\n# Normalize overlapping count by the number of grams in the headline\ntest_normalized_overlapping_counts = [oc / len(h) if len(h) > 0 else 0 for oc, h in zip(test_overlapping_counts, test_headline_unigrams)]\n\n# Create DataFrame for count features for test data\ncount_features_test = pd.DataFrame({\n    'headline_unique_counts': test_headline_unique_counts,\n    'body_unique_counts': test_body_unique_counts,\n    'ratio_counts': test_ratio_counts,\n    'overlapping_counts': test_overlapping_counts,\n    'normalized_overlapping_counts': test_normalized_overlapping_counts\n})","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:05:36.035035Z","iopub.execute_input":"2024-04-15T06:05:36.035715Z","iopub.status.idle":"2024-04-15T06:05:38.622087Z","shell.execute_reply.started":"2024-04-15T06:05:36.035674Z","shell.execute_reply":"2024-04-15T06:05:38.621242Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)\n\n# TF-IDF Vectorization for training data\nheadline_tfidf_train = tfidf_vectorizer.fit_transform(train_data['headline_processed'])\nbody_tfidf_train = tfidf_vectorizer.transform(train_data['body_processed'])\n\n# Compute cosine similarity between headline and body vectors for training data\ncosine_similarities_train = []\nfor headline_vec, body_vec in zip(headline_tfidf_train, body_tfidf_train):\n    cosine_similarities_train.append(cosine_similarity(headline_vec.reshape(1, -1), body_vec.reshape(1, -1))[0][0])\n\n# Create DataFrame for TF-IDF features for training data\ntfidf_features_train = pd.DataFrame({\n    'cosine_similarity': cosine_similarities_train\n})\n\nheadline_tfidf_valid = tfidf_vectorizer.fit_transform(valid_data['headline_processed'])\nbody_tfidf_valid = tfidf_vectorizer.transform(valid_data['body_processed'])\n\n# Compute cosine similarity between headline and body vectors for training data\ncosine_similarities_valid = []\nfor headline_vec, body_vec in zip(headline_tfidf_valid, body_tfidf_valid):\n    cosine_similarities_valid.append(cosine_similarity(headline_vec.reshape(1, -1), body_vec.reshape(1, -1))[0][0])\n\n# Create DataFrame for TF-IDF features for training data\ntfidf_features_valid = pd.DataFrame({\n    'cosine_similarity': cosine_similarities_valid\n})\n\n\n# TF-IDF Vectorization for testing data\nheadline_tfidf_test = tfidf_vectorizer.transform(test_data['headline_processed'])\nbody_tfidf_test = tfidf_vectorizer.transform(test_data['body_processed'])\n\n# Compute cosine similarity between headline and body vectors for testing data\ncosine_similarities_test = []\nfor headline_vec, body_vec in zip(headline_tfidf_test, body_tfidf_test):\n    cosine_similarities_test.append(cosine_similarity(headline_vec.reshape(1, -1), body_vec.reshape(1, -1))[0][0])\n\n# Create DataFrame for TF-IDF features for testing data\ntfidf_features_test = pd.DataFrame({\n    'cosine_similarity': cosine_similarities_test\n})","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:05:38.625061Z","iopub.execute_input":"2024-04-15T06:05:38.625370Z","iopub.status.idle":"2024-04-15T06:06:07.272061Z","shell.execute_reply.started":"2024-04-15T06:05:38.625339Z","shell.execute_reply":"2024-04-15T06:06:07.271033Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\n# Perform Singular Value Decomposition (SVD) on TF-IDF features for train dataset\nsvd_train = TruncatedSVD(n_components=100)  # Adjust the number of components as needed\nheadline_svd_train = svd_train.fit_transform(headline_tfidf_train)\nbody_svd_train = svd_train.transform(body_tfidf_train)\n\n# Compute cosine similarity between SVD features of headline and body for train dataset\ncosine_sim_svd_train = []\nfor h_svd, b_svd in zip(headline_svd_train, body_svd_train):\n    cosine_sim_svd_train.append(cosine_similarity(h_svd.reshape(1, -1), b_svd.reshape(1, -1))[0][0])\n\n# Create DataFrame for SVD features for train dataset\nsvd_features_train = pd.DataFrame({\n    'cosine_similarity_svd': cosine_sim_svd_train\n})\n\n# Perform Singular Value Decomposition (SVD) on TF-IDF features for train dataset\nsvd_valid = TruncatedSVD(n_components=100)  # Adjust the number of components as needed\nheadline_svd_valid = svd_valid.fit_transform(headline_tfidf_valid)\nbody_svd_valid = svd_valid.transform(body_tfidf_valid)\n\n# Compute cosine similarity between SVD features of headline and body for train dataset\ncosine_sim_svd_valid = []\nfor h_svd, b_svd in zip(headline_svd_valid, body_svd_valid):\n    cosine_sim_svd_valid.append(cosine_similarity(h_svd.reshape(1, -1), b_svd.reshape(1, -1))[0][0])\n\n# Create DataFrame for SVD features for train dataset\nsvd_features_valid = pd.DataFrame({\n    'cosine_similarity_svd': cosine_sim_svd_valid\n})\n\n# Perform Singular Value Decomposition (SVD) on TF-IDF features for test dataset\nsvd_test = TruncatedSVD(n_components=100)  # Adjust the number of components as needed\nheadline_svd_test = svd_test.fit_transform(headline_tfidf_test)\nbody_svd_test = svd_test.transform(body_tfidf_test)\n\n# Compute cosine similarity between SVD features of headline and body for test dataset\ncosine_sim_svd_test = []\nfor h_svd, b_svd in zip(headline_svd_test, body_svd_test):\n    cosine_sim_svd_test.append(cosine_similarity(h_svd.reshape(1, -1), b_svd.reshape(1, -1))[0][0])\n\n# Create DataFrame for SVD features for test dataset\nsvd_features_test = pd.DataFrame({\n    'cosine_similarity_svd': cosine_sim_svd_test\n})","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:06:07.273318Z","iopub.execute_input":"2024-04-15T06:06:07.273606Z","iopub.status.idle":"2024-04-15T06:06:15.893943Z","shell.execute_reply.started":"2024-04-15T06:06:07.273582Z","shell.execute_reply":"2024-04-15T06:06:15.893151Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Install Kaggle API\n!pip install kaggle\n\n# Set up Kaggle API credentials (replace 'username' and 'key' with your Kaggle username and API key)\n!mkdir ~/.kaggle\n!echo '{\"username\":\"joshidevanshi\",\"key\":\"4e038483c682fdbfb691f4fb95f5a416\"}' > ~/.kaggle/kaggle.json\n\nzip_file = \"googlenewsvectorsnegative300.zip\"\n\n# Check if the zip file already exists\nif not os.path.exists(zip_file):\n    # If the zip file does not exist, download the dataset using Kaggle API\n    # Download the Google News Word2Vec embeddings dataset\n    !kaggle datasets download -d leadbest/googlenewsvectorsnegative300\n\nelse:\n    print(\"The dataset zip file already exists.\")\n\n# Unzip the downloaded file\n# Check if the file exists\nif not os.path.exists('GoogleNews-vectors-negative300.bin'):\n    # If the file does not exist, unzip the downloaded file\n    !unzip googlenewsvectorsnegative300.zip\nelse:\n    print(\"The dataset is already unzipped.\")","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:06:15.895045Z","iopub.execute_input":"2024-04-15T06:06:15.895413Z","iopub.status.idle":"2024-04-15T06:07:37.603511Z","shell.execute_reply.started":"2024-04-15T06:06:15.895387Z","shell.execute_reply":"2024-04-15T06:07:37.601792Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Requirement already satisfied: kaggle in /opt/conda/lib/python3.10/site-packages (1.6.8)\nRequirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kaggle) (1.16.0)\nRequirement already satisfied: certifi>=2023.7.22 in /opt/conda/lib/python3.10/site-packages (from kaggle) (2024.2.2)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kaggle) (2.9.0.post0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kaggle) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kaggle) (4.66.1)\nRequirement already satisfied: python-slugify in /opt/conda/lib/python3.10/site-packages (from kaggle) (8.0.4)\nRequirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from kaggle) (1.26.18)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from kaggle) (6.1.0)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->kaggle) (0.5.1)\nRequirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.10/site-packages (from python-slugify->kaggle) (1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle) (3.6)\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\nDownloading googlenewsvectorsnegative300.zip to /kaggle/working\n100%|██████████████████████████████████████▊| 3.16G/3.17G [00:19<00:00, 166MB/s]\n100%|███████████████████████████████████████| 3.17G/3.17G [00:20<00:00, 170MB/s]\nArchive:  googlenewsvectorsnegative300.zip\n  inflating: GoogleNews-vectors-negative300.bin  \n  inflating: GoogleNews-vectors-negative300.bin.gz  \n","output_type":"stream"}]},{"cell_type":"code","source":"from gensim.models import KeyedVectors\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load pre-trained word vectors\nword_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n\n# Function to generate word vectors for a given text\ndef generate_word_vectors(text):\n    vectors = []\n    tokens = text.split()\n    for token in tokens:\n        if token in word_vectors:\n            vectors.append(word_vectors[token])\n    if vectors:\n        return np.mean(vectors, axis=0)\n    else:\n        return np.zeros(word_vectors.vector_size) \n\n# Generate word vectors for headline and body for train dataset\nheadline_word2vec_train = train_data['headline_processed'].apply(generate_word_vectors)\nbody_word2vec_train = train_data['body_processed'].apply(generate_word_vectors)\n\n# Compute cosine similarity between Word2Vec features of headline and body for train dataset\ncosine_sim_word2vec_train = []\nfor h_w2v, b_w2v in zip(headline_word2vec_train, body_word2vec_train):\n    cosine_sim_word2vec_train.append(cosine_similarity(h_w2v.reshape(1, -1), b_w2v.reshape(1, -1))[0][0])\n\n# Create DataFrame for Word2Vec features for train dataset\nword2vec_features_train = pd.DataFrame({\n    'cosine_similarity_word2vec': cosine_sim_word2vec_train\n})\n\n# Generate word vectors for headline and body for train dataset\nheadline_word2vec_valid = valid_data['headline_processed'].apply(generate_word_vectors)\nbody_word2vec_valid = valid_data['body_processed'].apply(generate_word_vectors)\n\n# Compute cosine similarity between Word2Vec features of headline and body for train dataset\ncosine_sim_word2vec_valid = []\nfor h_w2v, b_w2v in zip(headline_word2vec_valid, body_word2vec_valid):\n    cosine_sim_word2vec_valid.append(cosine_similarity(h_w2v.reshape(1, -1), b_w2v.reshape(1, -1))[0][0])\n\n# Create DataFrame for Word2Vec features for train dataset\nword2vec_features_valid = pd.DataFrame({\n    'cosine_similarity_word2vec': cosine_sim_word2vec_valid\n})\n\n# Generate word vectors for headline and body for test dataset\nheadline_word2vec_test = test_data['headline_processed'].apply(generate_word_vectors)\nbody_word2vec_test = test_data['body_processed'].apply(generate_word_vectors)\n\n# Compute cosine similarity between Word2Vec features of headline and body for test dataset\ncosine_sim_word2vec_test = []\nfor h_w2v, b_w2v in zip(headline_word2vec_test, body_word2vec_test):\n    cosine_sim_word2vec_test.append(cosine_similarity(h_w2v.reshape(1, -1), b_w2v.reshape(1, -1))[0][0])\n\n# Create DataFrame for Word2Vec features for test dataset\nword2vec_features_test = pd.DataFrame({\n    'cosine_similarity_word2vec': cosine_sim_word2vec_test\n})","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:07:37.606067Z","iopub.execute_input":"2024-04-15T06:07:37.606538Z","iopub.status.idle":"2024-04-15T06:09:02.528601Z","shell.execute_reply.started":"2024-04-15T06:07:37.606488Z","shell.execute_reply":"2024-04-15T06:09:02.527733Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from nltk.sentiment import SentimentIntensityAnalyzer\n\n# Function to calculate sentiment polarity scores for train dataset\ndef calculate_sentiment_train(text):\n    sid = SentimentIntensityAnalyzer()\n    sentiment_scores = sid.polarity_scores(text)\n    return sentiment_scores['compound']  # Using compound score as it combines positive, negative, and neutral scores\n\n# Calculate sentiment polarity scores for headline and body for train dataset\nheadline_sentiment_train = train_data['headline_processed'].apply(calculate_sentiment_train)\nbody_sentiment_train = train_data['body_processed'].apply(calculate_sentiment_train)\n\n# Create DataFrame for sentiment features for train dataset\nsentiment_features_train = pd.DataFrame({\n    'headline_sentiment': headline_sentiment_train,\n    'body_sentiment': body_sentiment_train\n})\n\ndef calculate_sentiment_valid(text):\n    sid = SentimentIntensityAnalyzer()\n    sentiment_scores = sid.polarity_scores(text)\n    return sentiment_scores['compound']  # Using compound score as it combines positive, negative, and neutral scores\n\n# Calculate sentiment polarity scores for headline and body for train dataset\nheadline_sentiment_valid = valid_data['headline_processed'].apply(calculate_sentiment_valid)\nbody_sentiment_valid = valid_data['body_processed'].apply(calculate_sentiment_valid)\n\n# Create DataFrame for sentiment features for train dataset\nsentiment_features_valid = pd.DataFrame({\n    'headline_sentiment': headline_sentiment_valid,\n    'body_sentiment': body_sentiment_valid\n})\n\n# Function to calculate sentiment polarity scores for test dataset\ndef calculate_sentiment_test(text):\n    sid = SentimentIntensityAnalyzer()\n    sentiment_scores = sid.polarity_scores(text)\n    return sentiment_scores['compound']  # Using compound score as it combines positive, negative, and neutral scores\n\n# Calculate sentiment polarity scores for headline and body for test dataset\nheadline_sentiment_test = test_data['headline_processed'].apply(calculate_sentiment_test)\nbody_sentiment_test = test_data['body_processed'].apply(calculate_sentiment_test)\n\n# Create DataFrame for sentiment features for test dataset\nsentiment_features_test = pd.DataFrame({\n    'headline_sentiment': headline_sentiment_test,\n    'body_sentiment': body_sentiment_test\n})","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:09:02.529794Z","iopub.execute_input":"2024-04-15T06:09:02.530087Z","iopub.status.idle":"2024-04-15T06:18:15.583146Z","shell.execute_reply.started":"2024-04-15T06:09:02.530063Z","shell.execute_reply":"2024-04-15T06:18:15.582143Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n# Concatenate all the generated features for train dataset\nall_features_train = pd.concat([count_features_train, tfidf_features_train, svd_features_train, word2vec_features_train, sentiment_features_train], axis=1)\n\n# Define features and labels for train dataset\nX_train = all_features_train\ny_train = train_data['stance_cat']\n\n# Construct LightGBM or XGBoost classifier\nif model_selected == \"lgb\":\n    base_model = lgb.LGBMClassifier()\nelse:\n    base_model = xgb.XGBClassifier()    \n\nfinal_model = GaussianNB()\n\nclf = StackingClassifier(\n    estimators=[(model_selected, base_model)], \n    final_estimator=final_model,\n    stack_method='predict_proba', \n    passthrough=False  \n)\n\n# Fit classifier on train dataset\nclf.fit(X_train, y_train)\n\n# Perform cross-validation on train dataset\nscores_train = cross_val_score(clf, X_train, y_train, cv=10)\nprint(\"Accuracy for Train Dataset:\", np.mean(scores_train))\n\n# Concatenate all the generated features for validation dataset\nall_features_valid = pd.concat([count_features_valid, tfidf_features_valid, svd_features_valid, word2vec_features_valid, sentiment_features_valid], axis=1)\n\n# Define features and labels for validation dataset\nX_valid = all_features_valid\ny_valid = valid_data['stance_cat']\n\n# Predict on validation dataset\ny_pred_valid = clf.predict(X_valid)\n\n# Calculate accuracy on validation dataset\naccuracy_valid = accuracy_score(y_valid, y_pred_valid)\nprint(\"Accuracy for Validation Dataset:\", accuracy_valid)\n\n# Concatenate all the generated features for test dataset\nall_features_test = pd.concat([count_features_test, tfidf_features_test, svd_features_test, word2vec_features_test, sentiment_features_test], axis=1)\n\n# Define features and labels for test dataset\nX_test = all_features_test\ny_test = test_data['stance_cat']\n\n# Predict on test dataset\ny_pred_test = clf.predict(X_test)\n\n# Calculate accuracy on test dataset\naccuracy_test = accuracy_score(y_test, y_pred_test)\nprint(\"Accuracy for Test Dataset:\", accuracy_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:18:15.584710Z","iopub.execute_input":"2024-04-15T06:18:15.584998Z","iopub.status.idle":"2024-04-15T06:19:10.328567Z","shell.execute_reply.started":"2024-04-15T06:18:15.584973Z","shell.execute_reply":"2024-04-15T06:19:10.327784Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Accuracy for Train Dataset: 0.8444814529818988\nAccuracy for Validation Dataset: 0.8544303797468354\nAccuracy for Test Dataset: 0.842830540037244\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\n\n# Train the XGBoost/LightGBoost classifier on train dataset\nclf.fit(X_train, y_train)\n\n# Make predictions on train dataset\ny_train_pred = clf.predict(X_train)\n\n# Calculate precision, recall, and F1-score for each class on train dataset\nprecision_train = precision_score(y_train, y_train_pred, average=None)\nrecall_train = recall_score(y_train, y_train_pred, average=None)\nf1_train = f1_score(y_train, y_train_pred, average=None)\n\n# Calculate macro-averaged precision, recall, and F1-score on train dataset\nmacro_precision_train = precision_score(y_train, y_train_pred, average='macro')\nmacro_recall_train = recall_score(y_train, y_train_pred, average='macro')\nmacro_f1_train = f1_score(y_train, y_train_pred, average='macro')\n\n# Print the results for train dataset\nprint(\"Train Dataset:\")\nprint(model_selected  + \" Accuracy:\", np.mean(scores_train))\nprint(\"Class 0 - Precision:\", precision_train[0], \", Recall:\", recall_train[0], \", F1-score:\", f1_train[0])\nprint(\"Class 1 - Precision:\", precision_train[1], \", Recall:\", recall_train[1], \", F1-score:\", f1_train[1])\nprint(\"Class 2 - Precision:\", precision_train[2], \", Recall:\", recall_train[2], \", F1-score:\", f1_train[2])\nprint(\"Class 3 - Precision:\", precision_train[3], \", Recall:\", recall_train[3], \", F1-score:\", f1_train[3])\nprint(\"Macro Precision:\", macro_precision_train)\nprint(\"Macro Recall:\", macro_recall_train)\nprint(\"Macro F1 Score:\", macro_f1_train)\n\n# Make predictions on test dataset\ny_valid_pred = clf.predict(X_valid)\n\n# Calculate precision, recall, and F1-score for each class on test dataset\nprecision_valid = precision_score(y_valid, y_valid_pred, average=None)\nrecall_valid = recall_score(y_valid, y_valid_pred, average=None)\nf1_valid = f1_score(y_valid, y_valid_pred, average=None)\n\n# Calculate macro-averaged precision, recall, and F1-score on test dataset\nmacro_precision_valid = precision_score(y_valid, y_valid_pred, average='macro')\nmacro_recall_valid = recall_score(y_valid, y_valid_pred, average='macro')\nmacro_f1_valid = f1_score(y_valid, y_valid_pred, average='macro')\n\n# Print the results for test dataset\nprint(\"\\nValidation Dataset:\")\nprint(\"Accuracy for Validation Dataset:\", accuracy_valid)\nprint(\"Class 0 - Precision:\", precision_valid[0], \", Recall:\", recall_valid[0], \", F1-score:\", f1_valid[0])\nprint(\"Class 1 - Precision:\", precision_valid[1], \", Recall:\", recall_valid[1], \", F1-score:\", f1_valid[1])\nprint(\"Class 2 - Precision:\", precision_valid[2], \", Recall:\", recall_valid[2], \", F1-score:\", f1_valid[2])\nprint(\"Class 3 - Precision:\", precision_valid[3], \", Recall:\", recall_valid[3], \", F1-score:\", f1_valid[3])\nprint(\"Macro Precision:\", macro_precision_valid)\nprint(\"Macro Recall:\", macro_recall_valid)\nprint(\"Macro F1 Score:\", macro_f1_valid)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:19:10.330037Z","iopub.execute_input":"2024-04-15T06:19:10.330633Z","iopub.status.idle":"2024-04-15T06:19:15.386338Z","shell.execute_reply.started":"2024-04-15T06:19:10.330603Z","shell.execute_reply":"2024-04-15T06:19:15.385436Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Train Dataset:\nxgb Accuracy: 0.8444814529818988\nClass 0 - Precision: 0.9439252336448598 , Recall: 0.8582596872875595 , F1-score: 0.8990564358198327\nClass 1 - Precision: 0.9171511627906976 , Recall: 0.9389880952380952 , F1-score: 0.9279411764705883\nClass 2 - Precision: 0.9290106951871657 , Recall: 0.9750245545110144 , F1-score: 0.9514616279865817\nClass 3 - Precision: 0.9940789473684211 , Recall: 0.9846397318935022 , F1-score: 0.9893368253671313\nMacro Precision: 0.9460415097477861\nMacro Recall: 0.9392280172325429\nMacro F1 Score: 0.9419490164110335\n\nValidation Dataset:\nAccuracy for Validation Dataset: 0.8544303797468354\nClass 0 - Precision: 0.6632302405498282 , Recall: 0.5244565217391305 , F1-score: 0.5857359635811836\nClass 1 - Precision: 0.45652173913043476 , Recall: 0.25 , F1-score: 0.32307692307692304\nClass 2 - Precision: 0.7795275590551181 , Recall: 0.8888888888888888 , F1-score: 0.8306240167802831\nClass 3 - Precision: 0.9669917479369843 , Recall: 0.9597915115413254 , F1-score: 0.9633781763826608\nMacro Precision: 0.7165678216680913\nMacro Recall: 0.6557842305423361\nMacro F1 Score: 0.6757037699552626\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support\ndef score_submission(gold_labels, test_labels):\n    score = 0.0\n\n    for i, (g, t) in enumerate(zip(gold_labels, test_labels)):\n        g_stance, t_stance = g, t\n        if g_stance == t_stance:\n            score += 0.25\n            if g_stance != 'unrelated':\n                score += 0.50\n        if g_stance in [0, 1, 2] and t_stance in [0, 1, 2]:\n            score += 0.25\n\n    return score\n\ndef report_score(actual,predicted):\n    score = score_submission(actual,predicted)\n    best_score = score_submission(actual,actual)\n\n    print(\"Score: \" +str(score) + \" out of \" + str(best_score) + \"\\t(\"+str(score*100/best_score) + \"%)\")\n    return score*100/best_score","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:19:15.387528Z","iopub.execute_input":"2024-04-15T06:19:15.387839Z","iopub.status.idle":"2024-04-15T06:19:15.618054Z","shell.execute_reply.started":"2024-04-15T06:19:15.387813Z","shell.execute_reply":"2024-04-15T06:19:15.617266Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Make predictions on test dataset\ny_test_pred = clf.predict(X_test)\n\n# Calculate precision, recall, and F1-score for each class on test dataset\nprecision_test = precision_score(y_test, y_test_pred, average=None)\nrecall_test = recall_score(y_test, y_test_pred, average=None)\nf1_test = f1_score(y_test, y_test_pred, average=None)\n\n# Calculate macro-averaged precision, recall, and F1-score on test dataset\nmacro_precision_test = precision_score(y_test, y_test_pred, average='macro')\nmacro_recall_test = recall_score(y_test, y_test_pred, average='macro')\nmacro_f1_test = f1_score(y_test, y_test_pred, average='macro')\n\n# Print the results for test dataset\nprint(\"\\nTest Dataset:\")\nprint(\"Accuracy for Test Dataset:\", accuracy_test)\nprint(\"Class 0 - Precision:\", precision_test[0], \", Recall:\", recall_test[0], \", F1-score:\", f1_test[0])\nprint(\"Class 1 - Precision:\", precision_test[1], \", Recall:\", recall_test[1], \", F1-score:\", f1_test[1])\nprint(\"Class 2 - Precision:\", precision_test[2], \", Recall:\", recall_test[2], \", F1-score:\", f1_test[2])\nprint(\"Class 3 - Precision:\", precision_test[3], \", Recall:\", recall_test[3], \", F1-score:\", f1_test[3])\nprint(\"Macro Precision:\", macro_precision_test)\nprint(\"Macro Recall:\", macro_recall_test)\nprint(\"Macro F1 Score:\", macro_f1_test)\n\ncustom_score = score_submission(y_test, y_test_pred)\nreport_sc = report_score(y_test, y_test_pred)\n\nprint(\"Custom Score:\", custom_score)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T06:19:15.619186Z","iopub.execute_input":"2024-04-15T06:19:15.619713Z","iopub.status.idle":"2024-04-15T06:19:15.670264Z","shell.execute_reply.started":"2024-04-15T06:19:15.619687Z","shell.execute_reply":"2024-04-15T06:19:15.669347Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"\nTest Dataset:\nAccuracy for Test Dataset: 0.842830540037244\nClass 0 - Precision: 0.6390728476821192 , Recall: 0.5244565217391305 , F1-score: 0.5761194029850747\nClass 1 - Precision: 0.4098360655737705 , Recall: 0.2976190476190476 , F1-score: 0.3448275862068965\nClass 2 - Precision: 0.7700693756194251 , Recall: 0.8720538720538721 , F1-score: 0.8178947368421052\nClass 3 - Precision: 0.9657273419649657 , Recall: 0.9448584202682563 , F1-score: 0.9551789077212806\nMacro Precision: 0.69617640771007\nMacro Recall: 0.6597469654200766\nMacro F1 Score: 0.6735051584388393\nScore: 2021.75 out of 2349.5\t(86.05022345179826%)\nCustom Score: 2021.75\n","output_type":"stream"}]}]}