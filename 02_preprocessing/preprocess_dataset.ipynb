{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used to lemmatize dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 Steps Involved \n",
    "1. Replace contractions: We convert the contraction versions of words into their original and formal form (e.g., “don’t” is converted to “do not”);\n",
    "2. Upper- to lower-case conversion: To guarantees the correlation within the features;\n",
    "3. URLs Removal: Since URLs that are included in the articles have no meaning, it is preferred to eliminate them from the text.;\n",
    "Special symbols Removal: Like punctuation, emojis, and other special characters.\n",
    "4. Punctuation removal\n",
    "5. stopword removal,\n",
    "6.  and stemming or lemmatization (Removed the inflectional morphemes like “ed”, “est”, “s”, and “ing” from their token stem. Ex: confirmed → “confirm” + “ -ed”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jayatiparwani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jayatiparwani/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "data = pd.read_csv('finaltest.csv')\n",
    "\n",
    "# Initialize WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to apply lemmatization\n",
    "def apply_lemmatization(text):\n",
    "    try:\n",
    "        tokens = word_tokenize(str(text))  # Ensure text is converted to string\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        return ' '.join(lemmatized_tokens)\n",
    "    except Exception as e:\n",
    "        print(\"Error during lemmatization:\", e)\n",
    "        return text  # Return original text in case of error\n",
    "\n",
    "# Apply lemmatization to 'Headline' and 'articleBody' columns\n",
    "data['Headline'] = data['Headline'].apply(apply_lemmatization)\n",
    "data['articleBody'] = data['articleBody'].apply(apply_lemmatization)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "data.to_csv('lemmatized_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['stance_cat'] = data['Stance'].map({'agree':0,'disagree':1,'discuss':2,'unrelated':3}).astype(int)\n",
    "data['Stance'].value_counts()\n",
    "#'agree':0,'disagree':1,'discuss':2,'unrelated':3\n",
    "data.to_csv('lemmatized_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
